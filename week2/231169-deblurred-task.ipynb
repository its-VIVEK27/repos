{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":579020,"sourceType":"datasetVersion","datasetId":270005}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport glob\nimport cv2\nimport torchvision.transforms as transforms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def double_conv(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True)\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((1500,1500)),\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class img_dataset(Dataset):\n    def __init__(self, img_dir_motion_blurred, img_dir_sharp, transform=None):\n        self.img_dir_motion = img_dir_motion_blurred\n        self.img_dir_sharp = img_dir_sharp\n        self.transform = transform\n        self.img_files_jpg_motion = sorted(glob.glob(img_dir_motion_blurred+'/*jpg'), key=lambda x: int(x.split('/')[-1].split('_')[0]))\n        self.img_files_JPG_motion = sorted(glob.glob(img_dir_motion_blurred+'/*JPG'), key=lambda x: int(x.split('/')[-1].split('_')[0]))\n\n        self.img_files_jpeg_motion = sorted(glob.glob(img_dir_motion_blurred+'/*jpeg'), key=lambda x: int(x.split('/')[-1].split('_')[0]))\n        self.img_files_motion = self.img_files_jpg_motion+self.img_files_jpeg_motion+self.img_files_JPG_motion\n        self.img_files_jpg_sharp = sorted(glob.glob(img_dir_sharp+'/*jpg'), key=lambda x: int(x.split('/')[-1].split('_')[0]))\n        self.img_files_JPG_sharp = sorted(glob.glob(img_dir_sharp+'/*JPG'), key=lambda x: int(x.split('/')[-1].split('_')[0]))\n\n        self.img_files_jpeg_sharp = sorted(glob.glob(img_dir_sharp+'/*jpeg'), key=lambda x: int(x.split('/')[-1].split('_')[0]))\n        self.img_files_sharp = self.img_files_jpg_sharp+self.img_files_jpeg_sharp+self.img_files_JPG_sharp\n    def __len__(self):\n        return len(self.img_files_motion)\n    def __getitem__(self, idx):\n#         images_jpg_motion = cv2.imread(self.img_files_jpg_motion)\n#         images_jpeg_motion = cv2.imread(self.img_files_jpeg_motion)\n#         images_jpg_sharp = cv2.imread(self.img_files_jpg_sharp)\n#         images_jpeg_sharp = cv2.imread(self.img_files_jpeg_sharp)\n        images_motion = cv2.imread(self.img_files_motion[idx])\n        images_sharp = cv2.imread(self.img_files_sharp[idx])\n        if self.transform:\n#                     images_jpg_motion = self.transform(images_jpg_motion)\n#                     images_jpeg_motion = self.transform(images_jpeg_motion)\n#                     images_jpg_sharp = self.ransform(images_jpg_sharp)\n#                     images_jpeg_sharp = self.transform(images_jpeg_sharp)\n            images_motion = self.transform(images_motion)\n            images_sharp = self.transform(images_sharp)\n        return images_motion, images_sharp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_dataset = img_dataset('/kaggle/input/blur-dataset/motion_blurred', '/kaggle/input/blur-dataset/sharp',transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_image1,first_image2  = img_dataset.__getitem__(images_dataset, 210)\nprint(first_image1.shape, first_image2.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(images_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(images_dataset.img_files_sharp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(images_dataset.img_files_motion)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = int(0.8* len(images_dataset))\ntest_size = len(images_dataset)-train_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_dataset,test_dataset = random_split(images_dataset,[train_size,test_size])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = 1,\n                                           shuffle = True)\n\n\ntest_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n                                           batch_size = 1,\n                                           shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cropping function to ensure matching sizes\ndef crop_tensor(tensor, target_tensor):\n    target_size = target_tensor.size(2)\n    tensor_size = tensor.size(2)\n    delta = tensor_size - target_size\n    delta = delta // 2\n    return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class convnn(nn.Module):\n    def __init__(self):\n        super(convnn, self).__init__()\n        self.layer1 = double_conv(3, 64)\n        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.layer2 = double_conv(64, 128)\n        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.layer3 = double_conv(128, 256)\n        self.max_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.layer4 = double_conv(256, 512)\n        self.max_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.layer5 = double_conv(512, 1024)\n        \n        self.up1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)\n        self.layer6 = double_conv(1024, 512)  # 512 (from upsampled layer5) + 512 (from layer4)\n\n        self.up2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n        self.layer7 = double_conv(512, 256)  # 256 (from upsampled layer6) + 256 (from layer3)\n        \n        self.up3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n        self.layer8 = double_conv(256, 128)  # 128 (from upsampled layer7) + 128 (from layer2)\n        \n        self.up4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n        self.layer9 = double_conv(128, 64)  # 64 (from upsampled layer8) + 64 (from layer1)\n        \n        self.layer10 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=1)\n\n    def forward(self, x):\n        out1 = self.layer1(x)\n        out2 = self.max_pool1(out1)\n\n        out3 = self.layer2(out2)\n        out4 = self.max_pool2(out3)\n\n        out5 = self.layer3(out4)\n        out6 = self.max_pool3(out5)\n\n        out7 = self.layer4(out6)\n        out8 = self.max_pool4(out7)\n\n        out9 = self.layer5(out8)\n        \n        out10 = self.up1(out9)\n        out10 = crop_tensor(out7, out10)\n        out10 = torch.cat((out10, out7), dim=1)\n        out11 = self.layer6(out10)\n\n        out12 = self.up2(out11)\n        out12 = crop_tensor(out5, out12)\n        out12 = torch.cat((out12, out5), dim=1)\n        out13 = self.layer7(out12)\n        \n        out14 = self.up3(out13)\n        out14 = crop_tensor(out3, out14)\n        out14 = torch.cat((out14, out3), dim=1)\n        out15 = self.layer8(out14)\n        \n        out16 = self.up4(out15)\n        out16 = crop_tensor(out1, out16)\n        out16 = torch.cat((out16, out1), dim=1)\n        out17 = self.layer9(out16)\n        \n        out18 = self.layer10(out17)\n\n        return out18\n\n# model = convnn(num_classes=1) \n# print(model)\n\n# image = train_dataset[0]\n# output = model(image)\n# print(output.shape)  # Should output: torch.Size([1, 1, 388, 388]) based on U-Net original paper\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q=iter(train_loader)\nx,y = next(q)\nx = x.to(device)\ny = y.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = convnn()\nmodel.to(device)\n\ncriterion = nn.MSELoss()\n\noptimizer = torch.optim.Adam(model.parameters(),lr = 0.001, weight_decay=0.005,) #momentum = 0.9)\n\ntotal_step = len(train_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion(model(x),y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(50):\n    for i,(images,labels) in enumerate(train_loader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        loss = criterion(outputs,labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print('Epoch [{}/{}], LOss: {:.4f}'.format(epoch+1,50,loss.item()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for x, y in train_loader:\n        x = x.to(device)\n        y = y.to(device)\n        outputs = model(images)\n        predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == y).sum().item()\n\n    print('Accuracy of the network on the {} train images: {} %'.format(70, 100 * correct / total))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w=iter(test_loader)\nx,y = next(w)\nx = x.to(device)\ny = y.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p=model(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_tensor = p\nimage_tensor = image_tensor.squeeze(0)\nimage_tensor = image_tensor.detach().cpu()\nimage_np = image_tensor.numpy()\nimage_np = np.transpose(image_np,(1,2,0))\nplt.imshow(image_np)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}